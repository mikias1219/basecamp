{
  "summary": "**Weekly Internship Report: 2025-04-28 to 2025-05-28**\n\n**Key Learnings:**\nThis week, the team made significant strides in understanding and applying large language models (LLMs) for AI-powered applications. A primary focus has been on fine-tuning LLaMA 2, exploring Retrieval-Augmented Generation (RAG) techniques, and experimenting with different model architectures. Efforts were made to optimize models through hyperparameter tuning, successfully identifying optimal learning rates and reducing validation loss. Data exploration and GPT-2 model training yielded promising initial results. The team also successfully integrated data fetching from Basecamp to power an AI recommendation system, creating a user interface to visualize the workflow.\n\n**Challenges Encountered:**\nA recurring challenge has been the limited computational resources available on Google Colab, particularly the 15GB GPU, which proved insufficient for effective fine-tuning of larger models like LLaMA 2, even with optimization techniques. Access restrictions to certain pre-trained LLaMA models on Hugging Face, requiring approvals, has also created delays. Additionally, data cleaning processes strained computational power. Request for access to LLaMA 3 and a server with greater GPU capacity have been submitted to mitigate these issues.\n",
  "insights": ""
}